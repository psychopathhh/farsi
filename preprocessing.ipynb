{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28632d06",
   "metadata": {},
   "source": [
    "# Notebook for configuring the project\n",
    "The project already contains files needed to run it by default in `data AM substitution/`, copy it's content to `data/` and project will be configured\n",
    "\n",
    "Below are instructions to change parts of this default configuration (most likely third section - adding more images is most useful for you)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0075ce8a",
   "metadata": {},
   "source": [
    "## Adding custom text\n",
    "If you are not satisfied with source text in `data AM substitution/newsgroup/newsgroup.txt` you can just directly edit it. The only thing that matters is the name of the folder and file - keep it the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf4138a",
   "metadata": {},
   "source": [
    "## Constructing necessary font models\n",
    "This cell looks at fonts, specified by `data/fonts/fontlist.txt` and constructs linear models needed for rendering them on images\n",
    "\n",
    "Run this after adding more fonts to `data/fonts/` to generate new font models at `data/models/font_px2pt.cp`\n",
    "\"Font models map each pixel of the font to related points, because points have physical measure of length while pixels dont.\" - say authors of original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56e9145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# Author: Ankush Gupta\n",
    "# Date: 2015\n",
    "\n",
    "\"\"\"\n",
    "THIS SCRIPT IS NOT GUARANTEED TO WORK \n",
    "\n",
    "Fonts and font_px2pt.cp in `data AM substitution/` were configured using this file, so it's probably working,\n",
    "but it's extremely hard to test\n",
    "\"\"\"\n",
    "\n",
    "import pygame\n",
    "from pygame import freetype\n",
    "from SynthTextCore.text_utils import FontState\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pickle as pkl\n",
    "\n",
    "\n",
    "pygame.init()\n",
    "\n",
    "\n",
    "ys = np.arange(8,200)\n",
    "A = np.c_[ys,np.ones_like(ys)]\n",
    "\n",
    "xs = []\n",
    "models = {} #linear model\n",
    "\n",
    "FS = FontState()\n",
    "# plt.figure()\n",
    "for i in range(len(FS.fonts)):\n",
    "\tprint(i)\n",
    "\tfont = freetype.Font(FS.fonts[i], size=12)\n",
    "\th = []\n",
    "\tfor y in ys:\n",
    "\t\th.append(font.get_sized_glyph_height(float(y)))\n",
    "\th = np.array(h)\n",
    "\tm,_,_,_ = np.linalg.lstsq(A,h)\n",
    "\tmodels[font.name] = m\n",
    "\txs.append(h)\n",
    "\n",
    "with open('data/models/font_px2pt.cp','wb') as f:\n",
    "\tpkl.dump(models,f)\n",
    "# plt.plot(xs,ys[i])\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7227a36",
   "metadata": {},
   "source": [
    "## Adding more source images\n",
    "This section downloads a huge amount of pre-processed images from URLs specified below\n",
    "(Alternatively, you can download it manually [here](https://academictorrents.com/details/2dba9518166cbd141534cbf381aa3e99a087e83c))\n",
    "\n",
    "Then merges three parts: depth (15.4GB), segmentation (7.2GB) and images (9GB) into one `data/dset.h5` file (82GB)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e579adb",
   "metadata": {},
   "source": [
    "This cell just prepares functions and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca53271e-351e-41ed-992f-32c47b137434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os, sys\n",
    "import wget, tarfile\n",
    "from SynthTextCore.common import *\n",
    "import os.path as osp\n",
    "from PIL import Image\n",
    "\n",
    "DATA_PATH = 'data'\n",
    "# path to the data-file, containing image, depth and segmentation:\n",
    "DB_FNAME = osp.join(DATA_PATH, 'dset.h5')\n",
    "\n",
    "# paths to the downloaded pre-processed data\n",
    "more_depth_path = osp.join(DATA_PATH,'depth.h5')\n",
    "more_seg_path = osp.join(DATA_PATH,'seg.h5')\n",
    "more_img_file_path = osp.join(DATA_PATH, 'bg_img')\n",
    "\n",
    "# url of the pre-processed data\n",
    "URL_IMG = 'http://thor.robots.ox.ac.uk/~vgg/data/scenetext/preproc/bg_img.tar.gz'\n",
    "URL_DEPTH = 'http://thor.robots.ox.ac.uk/~vgg/data/scenetext/preproc/depth.h5'\n",
    "URL_SEG = 'http://thor.robots.ox.ac.uk/~vgg/data/scenetext/preproc/seg.h5'\n",
    "\n",
    "# download the pre-processed data: background image, depth and segmentation\n",
    "def download_preproc():\n",
    "  if not osp.exists(more_img_file_path):\n",
    "    try:\n",
    "      colorprint(Color.BLUE,'\\tdownloading image-data (8.9G) from: '+URL_IMG,bold=True)\n",
    "      print()\n",
    "      sys.stdout.flush()\n",
    "      out_fname = 'bg_img.tar.gz'\n",
    "      wget.download(URL_IMG,out=out_fname)\n",
    "      tar = tarfile.open(out_fname)\n",
    "      tar.extractall()\n",
    "      tar.close()\n",
    "      os.remove(out_fname)\n",
    "      colorprint(Color.BLUE,'\\n\\tdata saved at:'+more_img_file_path,bold=True)\n",
    "      sys.stdout.flush()\n",
    "    except:\n",
    "      print (colorize(Color.RED,'Image-Data not found and have problems downloading.',bold=True))\n",
    "      sys.stdout.flush()\n",
    "      sys.exit(-1)\n",
    "  elif not osp.exists(more_seg_path):\n",
    "    try: \n",
    "      colorprint(Color.BLUE,'\\tdownloading segmentation-data (6.9G) from: '+URL_SEG,bold=True)\n",
    "      print()\n",
    "      sys.stdout.flush()\n",
    "      out_fname = 'seg.h5'\n",
    "      wget.download(URL_SEG,out=out_fname)\n",
    "      colorprint(Color.BLUE,'\\n\\tdata saved at:'+more_seg_path,bold=True)\n",
    "      sys.stdout.flush()\n",
    "    except:\n",
    "      print (colorize(Color.RED,'Segmentation-Data not found and have problems downloading.',bold=True))\n",
    "      sys.stdout.flush()\n",
    "      sys.exit(-1)\n",
    "  elif not osp.exists(more_depth_path):\n",
    "    try: \n",
    "      colorprint(Color.BLUE,'\\tdownloading depth-data (15G) from: '+URL_DEPTH,bold=True)\n",
    "      print()\n",
    "      sys.stdout.flush()\n",
    "      out_fname = 'depth.h5'\n",
    "      wget.download(URL_DEPTH,out=out_fname)\n",
    "      colorprint(Color.BLUE,'\\n\\tdata saved at:'+more_depth_path,bold=True)\n",
    "      sys.stdout.flush()\n",
    "    except:\n",
    "      print (colorize(Color.RED,'Depth-Data not found and have problems downloading.',bold=True))\n",
    "      sys.stdout.flush()\n",
    "      sys.exit(-1)\n",
    "\n",
    "\n",
    "# add/merge pre-processed data files into dset_8000.h5 \n",
    "def add_more_data_into_dset(DB_FNAME,more_img_file_path,more_depth_path,more_seg_path):\n",
    "  print (colorize(Color.GREEN,'adding data into h5 file..',bold=True))\n",
    "  # open files (a:append, r:read, w:write/overwrite)\n",
    "  db=h5py.File(DB_FNAME,'w')\n",
    "  depth_db=h5py.File(more_depth_path, 'r')\n",
    "  seg_db=h5py.File(more_seg_path, 'r')\n",
    "  db.create_group('image')\n",
    "  db.create_group('depth')\n",
    "  db.create_group('seg')\n",
    "\n",
    "  liist = os.listdir(more_img_file_path)\n",
    "  # Should be the only problematic images in the dataset\n",
    "  liist.remove('turtles_5.jpg')\n",
    "  liist.remove('hubble_44.jpg')\n",
    "  liist.remove('aquarium_126.jpg')\n",
    "  liist.remove('van+gogh_19.jpg')\n",
    "  iterator = tqdm(liist, desc=\"Constructing dset.h5\", total=len(liist))\n",
    "  try:\n",
    "    for imname in iterator:\n",
    "      if imname.endswith('.jpg'):\n",
    "        full_path=more_img_file_path + '\\\\' + imname\n",
    "        iterator.set_description(f\"'{full_path}': {os.path.getsize(full_path)} bytes\")\n",
    "\n",
    "        try:\n",
    "          img_np = np.array(Image.open(full_path))\n",
    "        except OSError:\n",
    "          continue\n",
    "\n",
    "        # specify exceptions, because not every image has a corresponding depth and segmentation \n",
    "        try:\n",
    "          db['depth'].create_dataset(imname,data=depth_db[imname])\n",
    "        except KeyError:\n",
    "          continue \n",
    "\n",
    "        try:\n",
    "          db['seg'].create_dataset(imname,data=seg_db['mask'][imname])\n",
    "          db['seg'][imname].attrs['area']=seg_db['mask'][imname].attrs['area']\n",
    "          db['seg'][imname].attrs['label']=seg_db['mask'][imname].attrs['label']\n",
    "        except KeyError:\n",
    "          continue\n",
    "\n",
    "        db['image'].create_dataset(imname,data=img_np)\n",
    "    print(colorize(Color.GREEN,'\\t-> done',bold=True))\n",
    "  except KeyboardInterrupt:\n",
    "    print(colorize(Color.RED,'\\tExiting early',bold=True))\n",
    "  finally:\n",
    "    db.close()\n",
    "    depth_db.close()\n",
    "    seg_db.close()\n",
    "    print (colorize(Color.BLUE,'Stored the data in: '+DB_FNAME, bold=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80888ea0",
   "metadata": {},
   "source": [
    "If you downloaded the dataset manually, make sure to set up the files like this:\n",
    "data/\n",
    "  - depth.h5\n",
    "  - seg.h5\n",
    "  - bg_img/\n",
    "    - img1.jpg\n",
    "    - img2.jpg\n",
    "    - ...\n",
    "\n",
    "Or else this script will try to download them anyway\n",
    "\n",
    "After initialization, run this cell.\n",
    "Make sure you have enough disk space! This script __should__ be safe in the sense that any data already saved will be fine, even if the script crashes in the middle of merging, but there might be something I did not catch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4577101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "  download_preproc() # won't do anything if data/ is set up like described above\n",
    "  add_more_data_into_dset(DB_FNAME,more_img_file_path,more_depth_path,more_seg_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
