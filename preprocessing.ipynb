{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28632d06",
   "metadata": {},
   "source": [
    "# Notebook for configuring the project\n",
    "The project already contains files needed to run it by default in `data AM substitution/`, copy it's content to `data/` and project will be configured\n",
    "\n",
    "Below are instructions to change parts of this default configuration (most likely third section - adding more images is most useful for you)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0075ce8a",
   "metadata": {},
   "source": [
    "## Adding custom text\n",
    "If you are not satisfied with source text in `data AM substitution/newsgroup/newsgroup.txt` you can just directly edit it. The only thing that matters is the name of the folder and file - keep it the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf4138a",
   "metadata": {},
   "source": [
    "## Constructing necessary font models\n",
    "This cell looks at fonts, specified by `data/fonts/fontlist.txt` and constructs linear models needed for rendering them on images\n",
    "\n",
    "Run this after adding more fonts to `data/fonts/` to generate new font models at `data/models/font_px2pt.cp`\n",
    "\"Font models map each pixel of the font to related points, because points have physical measure of length while pixels dont.\" - say authors of original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e56e9145",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "the STRING opcode argument must be quoted",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m xs = []\n\u001b[32m     26\u001b[39m models = {} \u001b[38;5;66;03m#linear model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m FS = \u001b[43mFontState\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# plt.figure()\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(FS.fonts)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Documents\\GitHub\\printed_text_ocr_generator_fa\\SynthTextCore\\text_utils.py:425\u001b[39m, in \u001b[36mFontState.__init__\u001b[39m\u001b[34m(self, data_dir)\u001b[39m\n\u001b[32m    423\u001b[39m     u = pickle._Unpickler(f)\n\u001b[32m    424\u001b[39m     u.encoding = \u001b[33m'\u001b[39m\u001b[33mlatin1\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m425\u001b[39m     p = \u001b[43mu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    426\u001b[39m     \u001b[38;5;28mself\u001b[39m.char_freq = p\n\u001b[32m    428\u001b[39m \u001b[38;5;66;03m# get the model to convert from pixel to font pt size:\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pickle.py:1256\u001b[39m, in \u001b[36m_Unpickler.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1254\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[32m   1255\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[32m-> \u001b[39m\u001b[32m1256\u001b[39m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1257\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[32m   1258\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst.value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pickle.py:1380\u001b[39m, in \u001b[36m_Unpickler.load_string\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1378\u001b[39m     data = data[\u001b[32m1\u001b[39m:-\u001b[32m1\u001b[39m]\n\u001b[32m   1379\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1380\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\u001b[33m\"\u001b[39m\u001b[33mthe STRING opcode argument must be quoted\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1381\u001b[39m \u001b[38;5;28mself\u001b[39m.append(\u001b[38;5;28mself\u001b[39m._decode_string(codecs.escape_decode(data)[\u001b[32m0\u001b[39m]))\n",
      "\u001b[31mUnpicklingError\u001b[39m: the STRING opcode argument must be quoted"
     ]
    }
   ],
   "source": [
    "# Author: Ankush Gupta\n",
    "# Date: 2015\n",
    "\n",
    "\"\"\"\n",
    "THIS SCRIPT IS NOT GUARANTEED TO WORK \n",
    "\n",
    "Fonts and font_px2pt.cp in `data AM substitution/` were configured using this file, so it's probably working,\n",
    "but it's extremely hard to test\n",
    "\"\"\"\n",
    "\n",
    "import pygame\n",
    "from pygame import freetype\n",
    "from SynthTextCore.text_utils import FontState\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pickle\n",
    "\n",
    "\n",
    "pygame.init()\n",
    "\n",
    "\n",
    "ys = np.arange(8,200)\n",
    "A = np.c_[ys,np.ones_like(ys)]\n",
    "\n",
    "xs = []\n",
    "models = {} #linear model\n",
    "\n",
    "FS = FontState()\n",
    "# plt.figure()\n",
    "for i in range(len(FS.fonts)):\n",
    "\tprint(i)\n",
    "\tfont = freetype.Font(FS.fonts[i], size=12)\n",
    "\th = []\n",
    "\tfor y in ys:\n",
    "\t\th.append(font.get_sized_glyph_height(float(y)))\n",
    "\th = np.array(h)\n",
    "\tm,_,_,_ = np.linalg.lstsq(A,h)\n",
    "\tmodels[font.name] = m\n",
    "\txs.append(h)\n",
    "\n",
    "with open('data/font_px2pt.cp', 'rb') as f:\n",
    "    u = pickle.Unpickler(f)\n",
    "    u.encoding = 'latin1'  # Note: Fixed typo from 'latini' to 'latin1'\n",
    "    try:\n",
    "        p = u.load()\n",
    "    except pickle.UnpicklingError:\n",
    "        # Try with different protocol\n",
    "        p = pickle.load(f, encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7227a36",
   "metadata": {},
   "source": [
    "## Adding more source images\n",
    "This section downloads a huge amount of pre-processed images from URLs specified below\n",
    "(Alternatively, you can download it manually [here](https://academictorrents.com/details/2dba9518166cbd141534cbf381aa3e99a087e83c))\n",
    "\n",
    "Then merges three parts: depth (15.4GB), segmentation (7.2GB) and images (9GB) into one `data/dset.h5` file (82GB)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e579adb",
   "metadata": {},
   "source": [
    "This cell just prepares functions and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca53271e-351e-41ed-992f-32c47b137434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os, sys\n",
    "import wget, tarfile\n",
    "from SynthTextCore.common import *\n",
    "import os.path as osp\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "DATA_PATH = 'data'\n",
    "# path to the data-file, containing image, depth and segmentation:\n",
    "DB_FNAME = osp.join(DATA_PATH, 'dset.h5')\n",
    "\n",
    "# paths to the downloaded pre-processed data\n",
    "more_depth_path = osp.join(DATA_PATH,'depth.h5')\n",
    "more_seg_path = osp.join(DATA_PATH,'seg.h5')\n",
    "more_img_file_path = osp.join(DATA_PATH, 'bg_img')\n",
    "\n",
    "# amount of images to save into dset.h5 (-1 == ALL)\n",
    "N_IMAGES_TO_ADD = 200\n",
    "\n",
    "# url of the pre-processed data\n",
    "URL_IMG = 'http://thor.robots.ox.ac.uk/~vgg/data/scenetext/preproc/bg_img.tar.gz'\n",
    "URL_DEPTH = 'http://thor.robots.ox.ac.uk/~vgg/data/scenetext/preproc/depth.h5'\n",
    "URL_SEG = 'http://thor.robots.ox.ac.uk/~vgg/data/scenetext/preproc/seg.h5'\n",
    "\n",
    "# download the pre-processed data: background image, depth and segmentation\n",
    "def download_preproc():\n",
    "  if not osp.exists(more_img_file_path):\n",
    "    try:\n",
    "      colorprint(Color.BLUE,'\\tdownloading image-data (8.9G) from: '+URL_IMG,bold=True)\n",
    "      print()\n",
    "      sys.stdout.flush()\n",
    "      out_fname = 'bg_img.tar.gz'\n",
    "      wget.download(URL_IMG,out=out_fname)\n",
    "      tar = tarfile.open(out_fname)\n",
    "      tar.extractall()\n",
    "      tar.close()\n",
    "      os.remove(out_fname)\n",
    "      colorprint(Color.BLUE,'\\n\\tdata saved at:'+more_img_file_path,bold=True)\n",
    "      sys.stdout.flush()\n",
    "    except:\n",
    "      print (colorize(Color.RED,'Image-Data not found and have problems downloading.',bold=True))\n",
    "      sys.stdout.flush()\n",
    "      sys.exit(-1)\n",
    "  elif not osp.exists(more_seg_path):\n",
    "    try: \n",
    "      colorprint(Color.BLUE,'\\tdownloading segmentation-data (6.9G) from: '+URL_SEG,bold=True)\n",
    "      print()\n",
    "      sys.stdout.flush()\n",
    "      out_fname = 'seg.h5'\n",
    "      wget.download(URL_SEG,out=out_fname)\n",
    "      colorprint(Color.BLUE,'\\n\\tdata saved at:'+more_seg_path,bold=True)\n",
    "      sys.stdout.flush()\n",
    "    except:\n",
    "      print (colorize(Color.RED,'Segmentation-Data not found and have problems downloading.',bold=True))\n",
    "      sys.stdout.flush()\n",
    "      sys.exit(-1)\n",
    "  elif not osp.exists(more_depth_path):\n",
    "    try: \n",
    "      colorprint(Color.BLUE,'\\tdownloading depth-data (15G) from: '+URL_DEPTH,bold=True)\n",
    "      print()\n",
    "      sys.stdout.flush()\n",
    "      out_fname = 'depth.h5'\n",
    "      wget.download(URL_DEPTH,out=out_fname)\n",
    "      colorprint(Color.BLUE,'\\n\\tdata saved at:'+more_depth_path,bold=True)\n",
    "      sys.stdout.flush()\n",
    "    except:\n",
    "      print (colorize(Color.RED,'Depth-Data not found and have problems downloading.',bold=True))\n",
    "      sys.stdout.flush()\n",
    "      sys.exit(-1)\n",
    "\n",
    "\n",
    "# add/merge pre-processed data files into dset_8000.h5 \n",
    "def add_more_data_into_dset(DB_FNAME,more_img_file_path,more_depth_path,more_seg_path):\n",
    "  print (colorize(Color.GREEN,'adding data into h5 file..',bold=True))\n",
    "  # open files (a:append, r:read, w:write/overwrite)\n",
    "  db=h5py.File(DB_FNAME,'w')\n",
    "  depth_db=h5py.File(more_depth_path, 'r')\n",
    "  seg_db=h5py.File(more_seg_path, 'r')\n",
    "  db.create_group('image')\n",
    "  db.create_group('depth')\n",
    "  db.create_group('seg')\n",
    "\n",
    "  liist = os.listdir(more_img_file_path)\n",
    "  # Should be the only problematic images in the dataset\n",
    "  liist.remove('turtles_5.jpg')\n",
    "  liist.remove('hubble_44.jpg')\n",
    "  liist.remove('aquarium_126.jpg')\n",
    "  liist.remove('van+gogh_19.jpg')\n",
    "  if 0 < N_IMAGES_TO_ADD < len(liist):\n",
    "    random.shuffle(liist) \n",
    "    liist = liist[:N_IMAGES_TO_ADD]\n",
    "\n",
    "  iterator = tqdm(liist, desc=\"Constructing dset.h5\", total=len(liist))\n",
    "  try:\n",
    "    for imname in iterator:\n",
    "      if imname.endswith('.jpg'):\n",
    "        full_path=more_img_file_path + '\\\\' + imname\n",
    "        iterator.set_description(f\"'{full_path}': {os.path.getsize(full_path)} bytes\")\n",
    "\n",
    "        try:\n",
    "          img_np = np.array(Image.open(full_path))\n",
    "        except OSError:\n",
    "          continue\n",
    "\n",
    "        # specify exceptions, because not every image has a corresponding depth and segmentation \n",
    "        try:\n",
    "          db['depth'].create_dataset(imname,data=depth_db[imname])\n",
    "        except KeyError:\n",
    "          continue \n",
    "\n",
    "        try:\n",
    "          db['seg'].create_dataset(imname,data=seg_db['mask'][imname])\n",
    "          db['seg'][imname].attrs['area']=seg_db['mask'][imname].attrs['area']\n",
    "          db['seg'][imname].attrs['label']=seg_db['mask'][imname].attrs['label']\n",
    "        except KeyError:\n",
    "          continue\n",
    "\n",
    "        db['image'].create_dataset(imname,data=img_np)\n",
    "    print(colorize(Color.GREEN,'\\t-> done',bold=True))\n",
    "  except KeyboardInterrupt:\n",
    "    print(colorize(Color.RED,'\\tExiting early',bold=True))\n",
    "  finally:\n",
    "    db.close()\n",
    "    depth_db.close()\n",
    "    seg_db.close()\n",
    "    print (colorize(Color.BLUE,'Stored the data in: '+DB_FNAME, bold=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80888ea0",
   "metadata": {},
   "source": [
    "If you downloaded the dataset manually, make sure to set up the files like this:\n",
    "data/\n",
    "  - depth.h5\n",
    "  - seg.h5\n",
    "  - bg_img/\n",
    "    - img1.jpg\n",
    "    - img2.jpg\n",
    "    - ...\n",
    "\n",
    "Or else this script will try to download them anyway\n",
    "\n",
    "After initialization, run this cell.\n",
    "Make sure you have enough disk space! This script __should__ be safe in the sense that any data already saved will be fine, even if the script crashes in the middle of merging, but there might be something I did not catch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4577101c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1madding data into h5 file..\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'data\\bg_img\\swan_86.jpg': 54675 bytes: 100%|██████████| 200/200 [00:28<00:00,  6.95it/s]               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\t-> done\u001b[0m\n",
      "\u001b[34;1mStored the data in: data\\dset.h5\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  download_preproc() # won't do anything if data/ is set up like described above\n",
    "  add_more_data_into_dset(DB_FNAME,more_img_file_path,more_depth_path,more_seg_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
